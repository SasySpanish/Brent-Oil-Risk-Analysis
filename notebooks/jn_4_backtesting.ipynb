{
  "metadata": {
    "kernelspec": {
      "name": "xpython",
      "display_name": "Python 3.13 (XPython)",
      "language": "python"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.13.1"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "b5dbef40-0c0f-4591-8cf2-6e788d2c8de2",
      "cell_type": "code",
      "source": "# Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom scipy.stats import chi2, norm\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nimport statsmodels.api as sm\n\nfrom arch import arch_model\nfrom sklearn.preprocessing import StandardScaler\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom quantile_forest import RandomForestQuantileRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n\n# Import dataset\ndata = pd.read_csv(\"brentv.csv\", parse_dates=[\"Date\"])\ndata.set_index(\"Date\", inplace=True)\ndata1 = pd.read_csv('brent.csv', parse_dates=[\"Date\"])\ndata1.set_index(\"Date\", inplace=True)\n\n# Log-returns\ndata[\"Log_Returns\"] = np.log(data[\"Close\"] / data[\"Close\"].shift(1))\ndata['Log_Returns'] = data['Log_Returns'].fillna(method='bfill')\nreturns = data['Log_Returns']\n\n# Calculate negative returns only (losses)\nreal_losses = returns.apply(lambda x: -x if x < 0 else 0)\n\n## Backtesting\n\n# === VaR Variables needed for Backtesting ===\n\n# Parametric VaR (95%)\nmean_return = np.mean(returns)\nstd_dev = np.std(returns)\nconfidence_level = 0.95\nz_score = norm.ppf(1 - confidence_level)\nVaR_parametric = mean_return + z_score * std_dev\n\n# Historical VaR (95%)\nVaR_historical = np.percentile(returns, (1 - confidence_level) * 100)\n\n# GARCH(1,1) model\ngarch = arch_model(returns, vol='GARCH', p=1, q=1)\nresults = garch.fit(disp='off')\nvolatility = results.conditional_volatility\n\n# Dynamic GARCH-based Parametric VaR\nVaRP_dynamic = mean_return + z_score * volatility\n\n# Dynamic GARCH-based Historical VaR\nstandardized_returns = returns / volatility\nstandard_p = np.percentile(standardized_returns, (1 - confidence_level) * 100)\nVaRH_dynamic = standard_p * volatility\n\n# Quantile Regression VaR\nX = data[['Close', 'Volume', 'Conditional_Volatility']]\ny = data['Log_Returns']\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX_scaled = sm.add_constant(X_scaled)\nregq = sm.QuantReg(y, X_scaled)\nresultreg = regq.fit(q=0.05)\nVaR_R = resultreg.predict(X_scaled)\n\n\n# ML Variables\n# Features and target\nX = data[['Close', 'Volume', 'Conditional_Volatility']]\ny = data['Log_Returns']\n\n# Train-Test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n\n# Standardization\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# --- Quantile Random Forest ---\nfrom sklearn.ensemble import RandomForestRegressor\nqrf = RandomForestRegressor(n_estimators=100, random_state=42)\nqrf.fit(X_train_scaled, y_train)\nVaR_QRF = np.percentile(qrf.predict(X_test_scaled), 5)  # 5th percentile (95% VaR)\n\n# --- Gradient Boosting ---\nfrom sklearn.ensemble import GradientBoostingRegressor\ngbr = GradientBoostingRegressor(loss=\"quantile\", alpha=0.05, random_state=42)\ngbr.fit(X_train_scaled, y_train)\nVaR_GB = gbr.predict(X_test_scaled)\n\n# --- XGBoost ---\nimport xgboost as xgb\nxgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\nxgb_model.fit(X_train_scaled, y_train)\nVaR_XGB = np.percentile(xgb_model.predict(X_test_scaled), 5)\n\n# --- LightGBM ---\nimport lightgbm as lgb\nlgb_model = lgb.LGBMRegressor(objective='quantile', alpha=0.05, random_state=42)\nlgb_model.fit(X_train_scaled, y_train)\nVaR_LGB = lgb_model.predict(X_test_scaled)\n\n# --- CatBoost ---\nfrom catboost import CatBoostRegressor\ncat_model = CatBoostRegressor(loss_function='Quantile:alpha=0.05', verbose=0, random_seed=42)\ncat_model.fit(X_train_scaled, y_train)\nVaR_CAT = cat_model.predict(X_test_scaled)\n\n# --- Neural Network ---\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\nnn = Sequential()\nnn.add(Dense(64, activation='relu', input_dim=X_train_scaled.shape[1]))\nnn.add(Dense(32, activation='relu'))\nnn.add(Dense(1, activation='linear'))\n\nnn.compile(optimizer='adam', loss='mean_squared_error')\nnn.fit(X_train_scaled, y_train, epochs=50, batch_size=32, verbose=0)\n\nVaR_NN = nn.predict(X_test_scaled)  # array shape (n,1)\n\n# Function to calculate VaR violations and Kupiec test results\ny_true = -real_losses\n\ndef calculate_var_violations(real_losses, models, alpha=0.05):\n    backtest1 = {}\n    n_obs = len(real_losses)\n\n    for model_name, var_values in models.items():\n        # Calculate violations\n        violations = -real_losses < var_values\n        n_violations = np.sum(violations)\n\n        # Hit Ratio\n        hit_ratio = n_violations / n_obs\n\n        # Kupiec Test\n        pof_stat = -2 * (n_obs * np.log(1 - alpha) + n_violations * np.log(alpha)) + 2 * (n_obs * np.log(1 - hit_ratio) + n_violations * np.log(hit_ratio))\n        p_value = 1 - chi2.cdf(pof_stat, df=1)\n\n        # Save results\n        backtest1[model_name] = {\n            'Hit Ratio': hit_ratio,\n            'Expected Hit Ratio': alpha,\n            'Kupiec Test Statistic': pof_stat,\n            'p-value': p_value\n        }\n\n    return backtest1\n\n# Functions to calculate performance metrics\ndef calculate_violations(y_true, y_pred):\n    return ((np.sum(y_true < y_pred)) / (len(y_true))) * 100\n\ndef calculate_coverage(y_true, y_pred):\n    return np.mean(y_true >= y_pred)\n\ndef quantile_loss(y_true, y_pred, alpha=0.05):\n    error = y_true - y_pred\n    return np.mean(np.where(error > 0, alpha * error, (alpha - 1) * error))\n\ndef calculate_mae(y_true, y_pred):\n    return mean_absolute_error(y_true, y_pred)\n\ndef calculate_rmse(y_true, y_pred):\n    return mean_squared_error(y_true, y_pred)\n\n# VaR Models\nmodels1 = {\n    'Parametric': VaR_parametric,\n    'Historical': VaR_historical,\n    'Parametric GARCH': VaRP_dynamic,\n    'Historical GARCH': VaRH_dynamic,\n    'Quantile Regression': VaR_R\n}\n\n# Transform scalar values into arrays\nscalar_models = ['Parametric', 'Historical']\n\nfor model_name in scalar_models:\n    if np.isscalar(models1[model_name]):  \n        models1[model_name] = np.full_like(y_true, models1[model_name])  \n\n# Results\nbacktesting1 = {}\n\nfor model_name, y_pred in models1.items():\n    var = y_pred.mean() * 100\n    violations = calculate_violations(y_true, y_pred)\n    kupiec = calculate_var_violations(real_losses, {model_name: y_pred})[model_name]['Kupiec Test Statistic']\n    coverage = calculate_coverage(y_true, y_pred)\n    q_loss = quantile_loss(y_true, y_pred)\n    mae = calculate_mae(y_true, y_pred)\n    rmse = calculate_rmse(y_true, y_pred)\n    \n    backtesting1[model_name] = {\n        'VaR': f\"{var:.2f}%\",\n        'Violations': f\"{violations:.2f}%\",\n        'Kupiec': f\"{kupiec:.4f}\",\n        'Coverage': f\"{coverage:.5f}\",\n        'MAE': f\"{mae:.5f}\",\n        'MSE': f\"{rmse:.5f}\"\n    }\n\n# Create a DataFrame to visualize results\nbacktesting1_df = pd.DataFrame(backtesting1).T\nbacktesting1_df['Model'] = backtesting1_df.index\n\n# Reorder columns to have 'Model' first\ncols = ['Model'] + [col for col in backtesting1_df.columns if col != 'Model']\nbacktesting1_df = backtesting1_df[cols]\n\n# Print results\nprint(backtesting1_df)\n\n# Subplot of the table\nfig, ax = plt.subplots(figsize=(8, 6))  \nax.axis('off')  \ntable = ax.table(cellText=backtesting1_df.values, colLabels=backtesting1_df.columns, loc='center')\nfor i in range(len(backtesting1_df.columns)):\n    max_len = max(backtesting1_df[backtesting1_df.columns[i]].astype(str).map(len).max(), len(backtesting1_df.columns[i]))  \n    table.auto_set_column_width([i])  \ntable.auto_set_font_size(False)\ntable.set_fontsize(10)\ntable.scale(1.2, 1.2)  \nplt.show()\n\n# Machine Learning Models\nmodels2 = {\n    'QR Forest': VaR_QRF,\n    'Gradient Boosting': VaR_GB,\n    'XGBoost': VaR_XGB,\n    'LightGBM': VaR_LGB,\n    'CatBoost': VaR_CAT,\n    'Neural Networks': VaR_NN[:, 0]\n}\n\n# Backtesting results\nbacktesting2 = {}\nprint(quantile_loss)\n\nfor model_name, y_pred in models2.items():\n    var = y_pred.mean() * 100\n    violations = calculate_violations(y_true, y_pred)\n    kupiec = calculate_var_violations(real_losses, {model_name: y_pred})[model_name]['Kupiec Test Statistic']\n    coverage = calculate_coverage(y_true, y_pred)\n    q_loss = quantile_loss(y_true, y_pred)\n    mae = calculate_mae(y_true, y_pred)\n    rmse = calculate_rmse(y_true, y_pred)\n    \n    backtesting2[model_name] = {\n        'VaR': f\"{var:.2f}%\",\n        'Violations': f\"{violations:.2f}%\",\n        'Kupiec': f\"{kupiec:.4f}\",\n        'Coverage': f\"{coverage:.5f}\",\n        'Quantile Loss': f\"{q_loss:.5f}\",\n        'MAE': f\"{mae:.5f}\",\n        'MSE': f\"{rmse:.5f}\"\n    }\n\n# Create a DataFrame to display the results\nbacktesting2_df = pd.DataFrame(backtesting2).T\nbacktesting2_df['Model'] = backtesting2_df.index\n\n# Reorder columns to have 'Model' as the first column\ncols2 = ['Model'] + [col for col in backtesting2_df.columns if col != 'Model']\nbacktesting2_df = backtesting2_df[cols]\n\n# Print results\nprint(backtesting2_df)\n\n# Subplot table\nfig, ax = plt.subplots(figsize=(8, 6))  # Set figure size\nax.axis('off')  # Hide axes\ntable = ax.table(cellText=backtesting2_df.values, colLabels=backtesting2_df.columns, loc='center')\nfor i in range(len(backtesting2_df.columns)):\n    max_len = max(backtesting2_df[backtesting2_df.columns[i]].astype(str).map(len).max(), len(backtesting2_df.columns[i]))  # Find max length\n    table.auto_set_column_width([i])  # Set automatic column width\ntable.auto_set_font_size(False)\ntable.set_fontsize(10)\ntable.scale(1.2, 1.2)  # Scale table size\nplt.show()\n\n# Losses\nplt.figure(figsize=(12, 6))\nplt.plot(data.index[1200:1500], real_losses[1200:1500], label=\"Observed Losses\", color=\"blue\", alpha=0.7)\nplt.plot(data.index[1200:1500], -VaR_GB[1200:1500]+0.005, label=\"95% VaR with Gradient Boosting\", color=\"orange\")\nplt.plot(data.index[1200:1500], -VaR_XGB[1200:1500]-0.008, label=\"95% VaR with XGB\", color=\"purple\")\nplt.plot(data.index[1200:1500], -VaR_LGB[1200:1500]-0.008, label=\"95% VaR with LGB\", color=\"cyan\", alpha=0.6)\nplt.plot(data.index[1200:1500], -VaR_CAT[1200:1500], label=\"95% VaR with CAT\", color=\"red\")\nplt.legend(loc='upper right')\nplt.show()\n\n# Returns\nplt.figure(figsize=(12, 6))\nplt.plot(data.index[1000:1500], data['Log_Returns'][1000:1500], label=\"Observed Returns\", color=\"blue\", alpha=0.7)\nplt.plot(data.index[1000:1500], VaR_GB[1000:1500]-0.005, label=\"95% VaR with Gradient Boosting\", color=\"orange\")\nplt.plot(data.index[1000:1500], VaR_XGB[1000:1500]+0.008, label=\"95% VaR with XGB\", color=\"purple\")\nplt.plot(data.index[1000:1500], VaR_LGB[1000:1500]+0.008, label=\"95% VaR with LGB\", color=\"cyan\", alpha=0.6)\nplt.plot(data.index[1000:1500], VaR_CAT[1000:1500], label=\"95% VaR with CAT\", color=\"red\")\nplt.plot(data.index[1000:1500], VaR_QRF[1000:1500], label=\"95% VaR with QRF\", color=\"lime\", alpha=0.4)\nplt.plot(data.index[1000:1500], VaR_NN[1000:1500]+0.003, label=\"95% VaR with NN\", color=\"mediumaquamarine\")\nplt.legend()\nplt.show()\n\n### Comparison of All Models\n\n# Models and their predictions\nmodels = {\n    'Parametric': VaR_parametric,\n    'Historical': VaR_historical,\n    'Parametric GARCH': VaRP_dynamic,\n    'Historical GARCH': VaRH_dynamic,\n    'Quantile Regression': VaR_R,\n    'QR Forest': VaR_QRF,\n    'Gradient Boosting': VaR_GB,\n    'XGBoost': VaR_XGB,\n    'LightGBM': VaR_LGB,\n    'CatBoost': VaR_CAT,\n    'Neural Networks': VaR_NN[:, 0]\n}\n\n# Check if the models in the list are scalars and transform them into arrays with the same length as y_true\nfor model_name in scalar_models:\n    if np.isscalar(models[model_name]):  # If the value is scalar\n        models[model_name] = np.full_like(y_true, models[model_name])  # Create an array filled with that value\n\n# Results\nresults = {}\n\nfor model_name, y_pred in models.items():\n    var = y_pred.mean() * 100\n    violations = calculate_violations(y_true, y_pred)\n    kupiec = calculate_var_violations(real_losses, {model_name: y_pred})[model_name]['Kupiec Test Statistic']\n    coverage = calculate_coverage(y_true, y_pred)\n    q_loss = quantile_loss(y_true, y_pred)\n    mae = calculate_mae(y_true, y_pred)\n    rmse = calculate_rmse(y_true, y_pred)\n    \n    results[model_name] = {\n        'VaR': f\"{var:.2f}%\",\n        'Violations': f\"{violations:.2f}%\",\n        'Kupiec': f\"{kupiec:.4f}\",\n        'Coverage': f\"{coverage:.5f}\",\n        'Quantile Loss': f\"{q_loss:.5f}\",\n        'MAE': f\"{mae:.5f}\",\n        'MSE': f\"{rmse:.5f}\"\n    }\n\n# Create a DataFrame to display the results\nresults_df = pd.DataFrame(results).T\nresults_df['Model'] = results_df.index\n\n# Reorder columns to have 'Model' as the first column\ncols = ['Model'] + [col for col in results_df.columns if col != 'Model']\nresults_df = results_df[cols]\n\n# Print results\nprint(results_df)\n\n# Table\nfig, ax = plt.subplots(figsize=(8, 6))  # Set figure size\nax.axis('off')  # Hide axes\ntable = ax.table(cellText=results_df.values, colLabels=results_df.columns, loc='center')\nfor i in range(len(results_df.columns)):\n    max_len = max(results_df[results_df.columns[i]].astype(str).map(len).max(), len(results_df.columns[i]))  # Find max length\n    table.auto_set_column_width([i])  # Set automatic column width\ntable.auto_set_font_size(False)\ntable.set_fontsize(10)\ntable.scale(1.2, 1.2)  # Scale table size\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}