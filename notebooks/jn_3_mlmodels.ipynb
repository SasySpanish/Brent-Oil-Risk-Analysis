{
  "metadata": {
    "kernelspec": {
      "name": "xpython",
      "display_name": "Python 3.13 (XPython)",
      "language": "python"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.13.1"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "1f449eab-fae0-4a9a-9432-975bf8af7a19",
      "cell_type": "code",
      "source": "# Libraries \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler\nfrom quantile_forest import RandomForestQuantileRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n# Import dataset\ndata = pd.read_csv(\"brentv.csv\", parse_dates=[\"Date\"])\ndata.set_index(\"Date\", inplace=True)\ndata1 = pd.read_csv(\"brent.csv\", parse_dates=[\"Date\"])\ndata1.set_index(\"Date\", inplace=True)\n\n# Log-returns\ndata[\"Log_Returns\"] = np.log(data[\"Close\"] / data[\"Close\"].shift(1))\ndata['Log_Returns'] = data['Log_Returns'].fillna(method='bfill')\nreturns = data['Log_Returns']\n\n# Calculate negative returns only (losses)\nreal_losses = returns.apply(lambda x: -x if x < 0 else 0)\n\n### Machine Learning Models for Value at Risk\n# Explanatory variables and target\nX = data[['Close', 'Volume', 'Conditional_Volatility']] # explanatory variables\ny = data['Log_Returns']  # target variable\n\n# Model structure\nqrf = RandomForestQuantileRegressor(\n    n_estimators=100, \n    max_depth=6, \n    min_samples_leaf=3, \n    default_quantiles=[0.05],\n    random_state=42\n)\n\n# Train the Quantile Regression Forest\nqrf.fit(X, y)\n\n# Compute the VaR using the lower quantile (5%)\nVaR_QRF = qrf.predict(X, quantiles=[0.05])\n\nprint(f\"95% VaR using Quantile Regression Forest: {VaR_QRF.mean():.2%}\")\n\n# Plot real losses vs VaR\nplt.figure(figsize=(12, 6))\nplt.plot(data.index[5400:], real_losses[5400:], label=\"Real losses\", color=\"lightsteelblue\")\nplt.plot(data.index[5400:], -VaR_QRF[5400:], label=\"95% VaR - Quantile Regression Forest\", color='green', alpha=0.5)\nplt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%Y'))  \nplt.gca().xaxis.set_major_locator(plt.matplotlib.dates.YearLocator())  \nplt.legend()\nplt.show()\n\n# Plot results\nplt.figure(figsize=(12, 6))\nplt.plot(data.index, data['Log_Returns'], label=\"Observed returns\", color=\"blue\", alpha=0.4)\nplt.plot(data.index, VaR_QRF, label=\"95% VaR - Quantile Regression Forest\", color=\"green\", alpha=0.6)\nplt.legend()\nplt.show()\n\n\n## Gradient Boosting\n# Quantile definition for VaR (e.g. 5% for 95% VaR)\nquantile = 0.05\n\n# Gradient Boosting model for the 5% quantile\ngbr = GradientBoostingRegressor(\n    loss='quantile', \n    alpha=0.05, \n    n_estimators=500, \n    learning_rate=0.01, \n    max_depth=3\n)\n\n# Train the model\ngbr.fit(X, y)\n\n# Predict VaR\nVaR_GB = gbr.predict(X)\nprint(f\"95% VaR using Gradient Boosting: {VaR_GB.mean():.2%}\")\n\n# Plot results\nplt.figure(figsize=(12, 6))\nplt.plot(data.index, data['Log_Returns'], label=\"Observed returns\", color=\"blue\", alpha=0.4)\nplt.plot(data.index, VaR_GB, label=\"95% VaR - Gradient Boosting\", color=\"orange\", alpha=0.8)\nplt.legend()\nplt.show()\n\n# Real losses vs VaR\nplt.figure(figsize=(12, 6))\nplt.plot(data.index[5400:], real_losses[5400:], label=\"Real losses\", color=\"lightsteelblue\")\nplt.plot(data.index[5400:], -VaR_GB[5400:], label=\"95% VaR - Gradient Boosting\", color='orange', alpha=0.5)\nplt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%Y'))  \nplt.gca().xaxis.set_major_locator(plt.matplotlib.dates.YearLocator())  \nplt.legend()\nplt.show()\n\n\n## XGBoost\nimport xgboost as xgb\n\n# Quantile regression model\nmodel_xgb = xgb.XGBRegressor(\n    objective='reg:quantileerror',\n    quantile_alpha=0.05, \n    n_estimators=100, \n    learning_rate=0.1, \n    max_depth=3\n)\n\n# Train the model\nmodel_xgb.fit(X, y)\n\n# Predictions\nVaR_XGB = model_xgb.predict(X)\nprint(f\"95% VaR using XGBoost: {VaR_XGB.mean():.2%}\")\n\n# Plot results\nplt.figure(figsize=(12, 6))\nplt.plot(data.index, data['Log_Returns'], label=\"Observed returns\", color=\"blue\", alpha=0.4)\nplt.plot(data.index, VaR_XGB, label=\"95% VaR - XGBoost\", color=\"purple\", alpha=0.8)\nplt.legend()\nplt.show()\n\n# Real losses vs VaR\nplt.figure(figsize=(12, 6))\nplt.plot(data.index[5400:], real_losses[5400:], label=\"Real losses\", color=\"lightsteelblue\")\nplt.plot(data.index[5400:], -VaR_XGB[5400:], label=\"95% VaR - XGBoost\", color=\"purple\", alpha=0.8)\nplt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%Y'))  \nplt.gca().xaxis.set_major_locator(plt.matplotlib.dates.YearLocator())  \nplt.legend()\nplt.show()\n\n\n## LightGBM\nimport lightgbm as lgb\n\n# Create LightGBM dataset\ntrain_data = lgb.Dataset(X, label=y)\n\n# LightGBM parameters for quantile regression\nparams = {\n    'objective': 'quantile',\n    'alpha': 0.05,\n    'metric': 'quantile',\n    'boosting_type': 'gbdt',\n    'num_leaves': 31,\n    'learning_rate': 0.1,\n    'feature_fraction': 0.9\n}\n\n# Train the model\nmodel_lgb = lgb.train(params, train_data, num_boost_round=100)\n\n# Predictions\nVaR_LGB = model_lgb.predict(X)\n\nprint(f\"95% VaR using LightGBM: {VaR_LGB.mean():.2%}\")\n\n# Plot results\nplt.figure(figsize=(12, 6))\nplt.plot(data.index, data['Log_Returns'], label=\"Observed returns\", color=\"blue\", alpha=0.4)\nplt.plot(data.index, VaR_LGB, label=\"95% VaR - LightGBM\", color=\"violet\", alpha=0.8)\nplt.legend()\nplt.show()\n\n# Real losses vs VaR\nplt.figure(figsize=(12, 6))\nplt.plot(data.index[5400:], real_losses[5400:], label=\"Real losses\", color=\"lightsteelblue\")\nplt.plot(data.index[5400:], -VaR_LGB[5400:], label=\"95% VaR - LightGBM\", color=\"violet\", alpha=0.8)\nplt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%Y'))  \nplt.gca().xaxis.set_major_locator(plt.matplotlib.dates.YearLocator())  \nplt.legend()\nplt.show()\n\n\n## CatBoost\nfrom catboost import CatBoostRegressor\n\n# CatBoost quantile regression\nmodel_cat = CatBoostRegressor(\n    loss_function='Quantile:alpha=0.05',\n    iterations=100, \n    learning_rate=0.1, \n    depth=3\n)\n\n# Train model\nmodel_cat.fit(X, y)\n\n# Predictions\nVaR_CAT = model_cat.predict(X)\n\nprint(f\"95% VaR using CatBoost: {VaR_CAT.mean():.2%}\")\n\n# Plot results\nplt.figure(figsize=(12, 6))\nplt.plot(data.index, data['Log_Returns'], label=\"Observed returns\", color=\"blue\", alpha=0.4)\nplt.plot(data.index, VaR_CAT, label=\"95% VaR - CatBoost\", color=\"deeppink\", alpha=0.8)\nplt.legend()\nplt.show()\n\n# Real losses vs VaR\nplt.figure(figsize=(12, 6))\nplt.plot(data.index[5400:], real_losses[5400:], label=\"Real losses\", color=\"lightsteelblue\")\nplt.plot(data.index[5400:], -VaR_CAT[5400:], label=\"95% VaR - CatBoost\", color=\"deeppink\", alpha=0.8)\nplt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%Y'))  \nplt.gca().xaxis.set_major_locator(plt.matplotlib.dates.YearLocator())  \nplt.legend()\nplt.show()\n\n\n## Neural Networks\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport random\n\n# Set seeds for reproducibility\nseed_value = 42\ntf.random.set_seed(seed_value)\nnp.random.seed(seed_value)\nrandom.seed(seed_value)\n\n# Explanatory variables and target\nX = data[['Close', 'Volume', 'Conditional_Volatility']] \ny = data['Log_Returns']  \n\n# Standardization\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\ny = scaler.fit_transform(y.to_numpy().reshape(-1, 1))\n\n# Quantile loss function\ndef quantile_loss(q):\n    def loss(y_true, y_pred):\n        error = y_true - y_pred\n        return tf.reduce_mean(tf.maximum(q * error, (q - 1) * error))\n    return loss\n\n# Neural Network model\nmodel = Sequential([\n    Dense(128, activation='relu', \n          kernel_regularizer=tf.keras.regularizers.l2(0.001), \n          input_shape=(X.shape[1],)),\n    BatchNormalization(),\n    Dropout(0.2),\n    \n    Dense(64, activation='relu', \n          kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n    BatchNormalization(),\n    Dropout(0.2),\n\n    Dense(32, activation='relu'),\n    Dense(1)\n])\n\n# Compile the model\nquantile = 0.05  \n\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate=0.002, decay_steps=100, decay_rate=0.9)\n\nmodel.compile(optimizer=Adam(learning_rate=lr_schedule), \n              loss=quantile_loss(quantile))\n\n# Early stopping\nearly_stopping = EarlyStopping(monitor='loss', patience=30, restore_best_weights=True)\n\n# Train the model\nhistory = model.fit(X, y, epochs=100, batch_size=64, callbacks=[early_stopping])\n\n# VaR predictions\nVaR_NN = scaler.inverse_transform(model.predict(X))\n\nprint(f\"95% VaR using Neural Networks: {VaR_NN.mean():.2%}\")\n\n# Training loss visualization\nplt.figure(figsize=(12, 6))\nplt.plot(history.history['loss'], label='Training Loss')\nplt.title('Loss evolution during training')\nplt.xlabel('Epochs')\nplt.ylabel('Quantile Loss')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot returns vs VaR\nplt.figure(figsize=(12, 6))\nplt.plot(data.index, data['Log_Returns'], label=\"Observed returns\", color=\"blue\", alpha=0.4)\nplt.plot(data.index, VaR_NN, label=\"95% VaR - Neural Networks\", color=\"mediumaquamarine\", alpha=0.8)\nplt.legend()\nplt.show()\n\n# Real losses vs VaR\nreturns = data['Log_Returns']\nreal_losses = returns.apply(lambda x: -x if x < 0 else 0)\n\nplt.figure(figsize=(12, 6))\nplt.plot(data.index[5400:], real_losses[5400:], label=\"Real losses\", color=\"lightsteelblue\")\nplt.plot(data.index[5400:], -VaR_NN[5400:], label=\"95% VaR - Neural Networks\", color=\"mediumaquamarine\", alpha=0.8)\nplt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%Y'))  \nplt.gca().xaxis.set_major_locator(plt.matplotlib.dates.YearLocator())  \nplt.legend()\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}